{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cd /kaggle","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install nltk\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install seqeval==0.0.3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install transformers","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom tqdm.notebook import tqdm\nimport torch\nfrom torch import nn\nfrom torch.utils import data\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler\nfrom torch.nn.utils.rnn import pad_sequence\nfrom transformers import BertModel, BertTokenizer, BertForTokenClassification, AdamW\nfrom seqeval.metrics import f1_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"input/5046mk1/train.csv\")\n\nall_sentences = train_df[\"Sentence\"]\nlabels = train_df[\"NER\"]\n\ntrain_df.head()\n\nval_df = pd.read_csv(\"input/5046mk1/val.csv\")\nval_sentences = val_df[\"Sentence\"]\nval_labels = val_df[\"NER\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pair_data = list(zip(all_sentences, labels))\nprint(pair_data[0])\n\npair_val = list(zip(val_sentences, val_labels))\nprint(pair_val[4])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"split_pair_data = []\nfor sent, sent_label in pair_data:\n    a_sent_pairs = []\n    a_sent_words = sent.split(\" \")\n    a_sent_labels = sent_label.split(\" \")\n    \n    temp = list(zip(a_sent_words, a_sent_labels))\n    split_pair_data.append(temp)\n    \n\nsplit_pair_val = []\nfor sent, sent_label in pair_val:\n    a_sent_pairs = []\n    a_sent_words = sent.split(\" \")\n    a_sent_labels = sent_label.split(\" \")\n    \n    temp = list(zip(a_sent_words, a_sent_labels))\n    split_pair_val.append(temp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"processed_datapair = []\nfor pair in split_pair_data:\n    tokenized_a_sentence_words = []\n    labels_wordpiece = []\n    \n    for a_word, word_label in pair:     \n        word_tokens = tokenizer.tokenize(a_word)\n        label_nums = len(word_tokens)\n        label_a_wordpiece = label_nums * [word_label]\n        \n        tokenized_a_sentence_words.extend(word_tokens)\n        labels_wordpiece.extend(label_a_wordpiece)\n    \n    processed_datapair.append([tokenized_a_sentence_words, labels_wordpiece])\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"processed_valpair = []\nfor pair in split_pair_val:\n    tokenized_a_sentence_words = []\n    labels_wordpiece = []\n    \n    for a_word, word_label in pair:     \n        word_tokens = tokenizer.tokenize(a_word)\n        label_nums = len(word_tokens)\n        label_a_wordpiece = label_nums * [word_label]\n        \n        tokenized_a_sentence_words.extend(word_tokens)\n        labels_wordpiece.extend(label_a_wordpiece)\n    \n    processed_valpair.append([tokenized_a_sentence_words, labels_wordpiece])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_labels = []\nfor pair in processed_datapair:\n    label = pair[1]\n    all_labels.append(label)\n    \nall_val_labels = []\nfor pair in processed_valpair:\n    label = pair[1]\n    all_val_labels.append(label)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(len(all_labels))\n# print(all_labels[0])\n\ni=0\ntemp = []\nfor sent_label in all_labels:\n#     print(sent_label)\n    temp.extend(sent_label)\n    \n#     i+=1\n#     if i == 3:\n#         break\n\nunique = list(set(temp))    \nprint(unique)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = []\nlabels.append(\"[PAD]\")\nlabels.extend(unique)\nlabels.append(\"[CLS]\")\nlabels.append(\"[SEP]\")\n\nlabels2idx = {labels:i for i, labels in enumerate(labels)}\nlabels2idx","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"idx2label = {value : key  for key, value in labels2idx.items()}\n\nidx2label\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokens_sent = []\nfor pair in processed_datapair:\n    tokens_sent.append(pair[0])\n\nprint(len(tokens_sent))\nprint(tokens_sent[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokens_sent_val = []\nfor pair in processed_valpair:\n    tokens_sent_val.append(pair[0])\n\nprint(len(tokens_sent_val))\nprint(tokens_sent_val[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_ids = []\nattention_masks = []\n\n# For every sentence...\nfor sent in tokens_sent:\n    # `encode_plus` will:\n    #   (1) Tokenize the sentence.\n    #   (2) Prepend the `[CLS]` token to the start.\n    #   (3) Append the `[SEP]` token to the end.\n    #   (4) Map tokens to their IDs.\n    #   (5) Pad or truncate the sentence to `max_length`\n    #   (6) Create attention masks for [PAD] tokens.\n    encoded_dict = tokenizer.encode_plus(\n                        sent,                      # Sentence to encode.\n                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n                        max_length = 75,           # Pad & truncate all sentences.\n                        pad_to_max_length = True,\n                        return_attention_mask = True,   # Construct attn. masks.\n                        return_tensors = 'pt',     # Return pytorch tensors.\n                   )\n    \n    # Add the encoded sentence to the list.    \n    input_ids.append(encoded_dict['input_ids'])\n    \n    # And its attention mask (simply differentiates padding from non-padding).\n    attention_masks.append(encoded_dict['attention_mask'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_ids_val = []\nattention_masks_val = []\n\n# For every sentence...\nfor sent in tokens_sent_val:\n    # `encode_plus` will:\n    #   (1) Tokenize the sentence.\n    #   (2) Prepend the `[CLS]` token to the start.\n    #   (3) Append the `[SEP]` token to the end.\n    #   (4) Map tokens to their IDs.\n    #   (5) Pad or truncate the sentence to `max_length`\n    #   (6) Create attention masks for [PAD] tokens.\n    encoded_dict = tokenizer.encode_plus(\n                        sent,                      # Sentence to encode.\n                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n                        max_length = 75,           # Pad & truncate all sentences.\n                        pad_to_max_length = True,\n                        return_attention_mask = True,   # Construct attn. masks.\n                        return_tensors = 'pt',     # Return pytorch tensors.\n                   )\n    \n    # Add the encoded sentence to the list.    \n    input_ids_val.append(encoded_dict['input_ids'])\n    \n    # And its attention mask (simply differentiates padding from non-padding).\n    attention_masks_val.append(encoded_dict['attention_mask'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(input_ids_val)\nprint(input_ids_val[0].size())\nprint(input_ids_val[5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels_idx = []\n\nfor pair in processed_datapair:\n    one_label_idx = []\n    \n    for label in pair[1]:\n        one_label_idx.append(labels2idx[label])\n\n    labels_idx.append(one_label_idx)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(labels_idx[0])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels_idx_val = []\n\nfor pair in processed_valpair:\n    one_label_idx = []\n    \n    for label in pair[1]:\n        one_label_idx.append(labels2idx[label])\n\n    labels_idx_val.append(one_label_idx)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_len = 75\nj = 0\nlabel_pad = []\n\nfor label in labels_idx:\n    label.insert(0, 6)\n    label.append(7)\n    if len(label) > max_len:\n        temp = label[0:max_len]\n        print(\"已经裁剪了\"+str(j))\n    else:\n        diff = max_len - len(label)\n        pad = diff*[0]\n        temp = label\n        temp.extend(pad)\n    \n    j += 1\n    label_pad.append(temp)\n        \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_len = 75\nj = 0\nlabel_pad_val = []\n\nfor label in labels_idx_val:\n    label.insert(0, 6)\n    label.append(7)\n    if len(label) > max_len:\n        temp = label[0:max_len]\n        print(\"已经裁剪了\"+str(j))\n    else:\n        diff = max_len - len(label)\n        pad = diff*[0]\n        temp = label\n        temp.extend(pad)\n    \n    j += 1\n    label_pad_val.append(temp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Wordpiece Label for NER\n#https://github.com/google-research/bert/issues/560\n\nclass Nerdata(data.Dataset):\n    def __init__(self, input_ids, attmask, labels):\n        self.data = input_ids\n        self.attmask = attmask\n        self.labels = labels\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        data = self.data[idx]\n        mask = self.attmask[idx]\n        label = self.labels[idx]\n        label = torch.tensor(label)\n        \n        return data, mask, label\n        \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = Nerdata(input_ids, attention_masks ,label_pad)\n\nprint(train_data.__len__())\n\nval_data = Nerdata(input_ids_val, attention_masks_val, label_pad_val)\nprint(val_data.__len__())\n\nprint(type(label_pad))\nprint(label_pad[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = BertForTokenClassification.from_pretrained('bert-large-cased', num_labels = 8, output_hidden_states = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dataloader_test = data.DataLoader(train_data, batch_size=2)\n\n# for batches in tqdm(dataloader_test):\n#     input_data = batches[0].view(-1,75)\n#     mask = batches[1].view(-1,75)\n#     label = batches[2]\n#     loss, logits = model(input_data, token_type_ids=None,attention_mask=mask, labels=label)\n#     break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 32\ndataloader_train = data.DataLoader(train_data, batch_size=batch_size)\ndataloader_val = data.DataLoader(val_data, batch_size=batch_size)\n\nbatch_num_val = len(dataloader_val)\nbatch_num_train = len(dataloader_train)\nprint(batch_num_val)\nprint(batch_num_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_epoch = 25\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nparam_optimizer = list(model.named_parameters())\nno_decay = ['bias', 'gamma', 'beta']\noptimizer_grouped_parameters = [\n        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n         'weight_decay_rate': 0.01},\n        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n         'weight_decay_rate': 0.0}]\n\noptimizer = AdamW(\n    optimizer_grouped_parameters,\n    lr=5e-5,\n    eps=1e-8\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_acc = 97\n\n######################################  Train the model\nmodel.cuda()\nfor epoch in range(total_epoch):\n    train_acc_epoch = 0\n    val_acc_epoch = 0\n    predictions , true_labels = [], []\n    predictions_val , true_labels_val = [], []\n\n    for batches in tqdm(dataloader_train):\n        \n        input_data = batches[0].view(-1,75).cuda()\n        mask = batches[1].view(-1,75).cuda()\n        label = batches[2].cuda()\n        \n        model.train()\n#         outputs = model(input_data, attention_masks = mask, labels=label)\n        loss, logits, embeddings = model(input_data, token_type_ids=None,\n                        attention_mask=mask, labels=label)\n\n        loss_train =  loss\n        loss_train.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        \n        \n        with torch.no_grad():\n            # Forward pass, calculate logit predictions.\n            # This will return the logits rather than the loss because we have not provided labels.\n            outputs = model(input_data, token_type_ids=None,\n                        attention_mask=mask, labels=label)\n            \n        # Move logits and labels to CPU\n        logits = outputs[1]\n        y_hat = nn.functional.softmax(logits, dim=2)\n        y_hat = torch.argmax(y_hat, dim=2)\n   \n        predictions.extend([list(p) for p in y_hat.cpu().detach().numpy()])\n        true_labels.extend([list(q) for q in label.cpu().detach().numpy()])\n        \n    pred_tags = [idx2label[p_i] for p, l in zip(predictions, true_labels)\n                                 for p_i, l_i in zip(p, l) if idx2label[l_i] != \"[PAD]\"]\n    valid_tags = [idx2label[l_i] for l in true_labels\n                                  for l_i in l if idx2label[l_i] != \"[PAD]\"]        \n        \n    print(\"train F1-Score: {0:.4}%\".format(f1_score(pred_tags, valid_tags)*100))\n\n    ##################################################################################################\n    for batches in tqdm(dataloader_val):\n        input_data = batches[0].view(-1,75).cuda()\n        mask = batches[1].view(-1,75).cuda()\n        label = batches[2].cuda()\n        \n        with torch.no_grad():\n            # Forward pass, calculate logit predictions.\n            # This will return the logits rather than the loss because we have not provided labels.\n            outputs = model(input_data, token_type_ids=None,\n                        attention_mask=mask, labels=label)\n        # Move logits and labels to CPU\n\n        logits = outputs[1]\n\n        y_hat = nn.functional.softmax(logits, dim=2)\n        y_hat = torch.argmax(y_hat, dim=2)\n   \n        predictions_val.extend([list(p) for p in y_hat.cpu().detach().numpy()])\n        true_labels_val.extend([list(q) for q in label.cpu().detach().numpy()])\n        \n    pred_tags_val = [idx2label[p_i] for p, l in zip(predictions_val, true_labels_val)\n                                 for p_i, l_i in zip(p, l) if idx2label[l_i] != \"[PAD]\"]\n    valid_tags_val = [idx2label[l_i] for l in true_labels_val\n                                  for l_i in l if idx2label[l_i] != \"[PAD]\"]        \n        \n    print(\"val F1-Score: {0:.4}%\".format(f1_score(pred_tags_val, valid_tags_val)*100))\n    \n    if (f1_score(pred_tags_val, valid_tags_val)*100) > max_acc:\n        torch.save(model, '/kaggle/working/best_model.' + str((f1_score(pred_tags_val, valid_tags_val)*100)) + 'pth')\n\n\n# torch.save(model, \"/kaggle/working/last\"+ str(epoch+1)+ \"_\" + str((f1_score(pred_tags_val, valid_tags_val)*100)) + \".pth\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(embeddings)\nlen(embeddings[1])\nembeddings[0].size()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataloader_embd = data.DataLoader(train_data, batch_size=1)\n\nmodel_bert = torch.load('/kaggle/working/best_model.pth')\n\ntrain_embds = []\nfor batches in tqdm(dataloader_embd):\n    input_data = batches[0].view(-1,75).cuda()\n    mask = batches[1].view(-1,75).cuda()\n    label = batches[2].cuda()\n        \n    with torch.no_grad():\n            # Forward pass, calculate logit predictions.\n            # This will return the logits rather than the loss because we have not provided labels.\n        outputs = model(input_data, token_type_ids=None,\n                        attention_mask=mask, labels=label)\n        # Move logits and labels to CPU\n    logits = outputs[2][0]\n    \n    train_embds.append(logits)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataloader_embd_val = data.DataLoader(val_data, batch_size=1)\n\nmodel_bert = torch.load('/kaggle/working/best_model.pth')\n\nval_embds = []\nfor batches in tqdm(dataloader_embd_val):\n    input_data = batches[0].view(-1,75).cuda()\n    mask = batches[1].view(-1,75).cuda()\n    label = batches[2].cuda()\n        \n    with torch.no_grad():\n            # Forward pass, calculate logit predictions.\n            # This will return the logits rather than the loss because we have not provided labels.\n        outputs = model(input_data, token_type_ids=None,\n                        attention_mask=mask, labels=label)\n        # Move logits and labels to CPU\n    logits = outputs[2][0]\n    \n    val_embds.append(logits)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_embds\n\nval_embds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}